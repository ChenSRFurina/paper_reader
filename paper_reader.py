import requests
import json
import os
import datetime
import time
from typing import TypedDict, List
from urllib.parse import urljoin
from bs4 import BeautifulSoup
import os
from dotenv import load_dotenv
# è‡ªåŠ¨åŠ è½½ .env æ–‡ä»¶
load_dotenv()

from openai import OpenAI
import re

# å®šä¹‰çŠ¶æ€ç±»å‹
class PPTState(TypedDict):
    content_url: str
    scraped_text: str
    image_urls: List[str]
    text_summary: str
    paper_title: str  # æ–°å¢è®ºæ–‡æ ‡é¢˜å­—æ®µ
    png_images: List[str]  # æ–°å¢PNGå›¾ç‰‡å­—æ®µ
    temp_filename: str  # æ–°å¢ä¸´æ—¶æ–‡ä»¶åå­—æ®µ

# åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ï¼ˆå…¼å®¹Qwen APIï¼‰
client = OpenAI(
    api_key=os.getenv("QWEN_API_KEY"),
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
)

def sanitize_filename(filename: str) -> str:
    """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦"""
    # ç§»é™¤æˆ–æ›¿æ¢éæ³•å­—ç¬¦
    filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
    # ç§»é™¤å¤šä½™çš„ç©ºæ ¼å’Œç‚¹
    filename = re.sub(r'\s+', ' ', filename).strip()
    # é™åˆ¶é•¿åº¦
    if len(filename) > 100:
        filename = filename[:100]
    return filename

def extract_paper_title(text_content: str, url: str) -> str:
    """ä»æ–‡æœ¬å†…å®¹æˆ–URLä¸­æå–è®ºæ–‡æ ‡é¢˜"""
    title = "æœªçŸ¥è®ºæ–‡"
    
    try:
        # æ–¹æ³•1ï¼šä»URLä¸­æå–ï¼ˆé€‚ç”¨äºarXivï¼‰
        if 'arxiv.org' in url:
            arxiv_patterns = [
                r'/abs/(\d+\.\d+)',
                r'/html/(\d+\.\d+)',
                r'/(\d+\.\d+)v?\d*'
            ]
            
            for pattern in arxiv_patterns:
                arxiv_match = re.search(pattern, url)
                if arxiv_match:
                    title = f"arXiv_{arxiv_match.group(1)}"
                    print(f"ä»URLæå–åˆ°arXivç¼–å·ï¼š{arxiv_match.group(1)}")
                    break
        
        # æ–¹æ³•2ï¼šä»æ–‡æœ¬å†…å®¹ä¸­æå–æ ‡é¢˜
        if title == "æœªçŸ¥è®ºæ–‡":
            # æ”¹è¿›çš„æ ‡é¢˜æå–æ¨¡å¼
            title_patterns = [
                r'Title[:\s]*([^\n\r]+)',
                r'æ ‡é¢˜[:\s]*([^\n\r]+)',
                r'^([A-Z][^.!?\n\r]{10,100})',  # ä»¥å¤§å†™å­—æ¯å¼€å¤´çš„é•¿å¥å­
                r'^([^.!?\n\r]{15,80})',  # é•¿åº¦é€‚ä¸­çš„ç¬¬ä¸€è¡Œ
                r'BannerAgency[^.!?\n\r]*',  # ç‰¹å®šå…³é”®è¯åŒ¹é…
                r'GPT-[^.!?\n\r]*',  # GPTç›¸å…³è®ºæ–‡
                r'Attention[^.!?\n\r]*',  # Attentionç›¸å…³è®ºæ–‡
            ]
            
            lines = text_content.split('\n')
            print(f"æ£€æŸ¥å‰10è¡Œæ–‡æœ¬å†…å®¹ï¼š")
            for i, line in enumerate(lines[:10]):
                line = line.strip()
                print(f"  è¡Œ{i+1}: {line[:50]}...")
                
                if len(line) > 10 and len(line) < 200:  # åˆç†çš„æ ‡é¢˜é•¿åº¦
                    for pattern in title_patterns:
                        match = re.search(pattern, line, re.IGNORECASE)
                        if match:
                            potential_title = match.group(0) if pattern.endswith('*') else match.group(1)
                            potential_title = potential_title.strip()
                            if len(potential_title) > 5:  # ç¡®ä¿æ ‡é¢˜æœ‰æ„ä¹‰
                                title = potential_title
                                print(f"æ‰¾åˆ°æ ‡é¢˜ï¼š{title}")
                                break
                    if title != "æœªçŸ¥è®ºæ–‡":
                        break
        
        # æ–¹æ³•3ï¼šä»HTMLæ ‡é¢˜æ ‡ç­¾ä¸­æå–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if title == "æœªçŸ¥è®ºæ–‡" and 'arxiv.org' in url:
            try:
                # å°è¯•è·å–HTMLé¡µé¢çš„æ ‡é¢˜
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                }
                response = requests.get(url, headers=headers, timeout=5)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    html_title = soup.find('title')
                    if html_title and html_title.text:
                        title_text = html_title.text.strip()
                        if len(title_text) > 5 and len(title_text) < 200:
                            title = title_text
                            print(f"ä»HTMLæ ‡é¢˜æå–ï¼š{title}")
            except Exception as e:
                print(f"ä»HTMLæ ‡é¢˜æå–å¤±è´¥ï¼š{e}")
        
        # æ¸…ç†æ ‡é¢˜
        title = sanitize_filename(title)
        print(f"æœ€ç»ˆæå–çš„æ ‡é¢˜ï¼š{title}")
        
    except Exception as e:
        print(f"æå–æ ‡é¢˜æ—¶å‡ºé”™ï¼š{e}")
        title = "æœªçŸ¥è®ºæ–‡"
    
    return title

def qwen_chat(messages):
    """ä½¿ç”¨Qwen LLMè¿›è¡Œå¯¹è¯"""
    try:
        # ç¡®ä¿æ¶ˆæ¯æ ¼å¼æ­£ç¡®
        formatted_messages = []
        for msg in messages:
            if isinstance(msg, dict) and "role" in msg and "content" in msg:
                formatted_messages.append(msg)
            else:
                print(f"è·³è¿‡æ— æ•ˆæ¶ˆæ¯æ ¼å¼: {msg}")
                continue
        
        if not formatted_messages:
            return "é”™è¯¯ï¼šæ²¡æœ‰æœ‰æ•ˆçš„æ¶ˆæ¯æ ¼å¼"
        
        # è°ƒç”¨Qwen API
        response = client.chat.completions.create(
            model="qwen-plus",
            messages=formatted_messages,
            temperature=0.7,
            max_tokens=4000,
        )
        
        # æå–å›å¤å†…å®¹
        if response.choices and len(response.choices) > 0:
            return response.choices[0].message.content
        else:
            return "é”™è¯¯ï¼šAPIè¿”å›ç©ºå“åº”"
            
    except Exception as e:
        print(f"Qwen LLMè°ƒç”¨å¤±è´¥ï¼š{e}")
        return f"LLMè°ƒç”¨å¤±è´¥ï¼š{e}"

def web_scraper(state: PPTState):
    """çˆ¬å–URLçš„æ–‡å­—å†…å®¹å’Œå›¾ç‰‡URL"""
    url = state["content_url"]
    
    try:
        # å‘é€HTTPè¯·æ±‚
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        # è§£æHTML
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # æå–æ–‡å­—å†…å®¹ï¼ˆå»é™¤scriptå’Œstyleæ ‡ç­¾ï¼‰
        for script in soup(["script", "style"]):
            script.decompose()
        
        # è·å–æ‰€æœ‰æ–‡æœ¬å†…å®¹
        text_content = soup.get_text()
        # æ¸…ç†æ–‡æœ¬ï¼ˆå»é™¤å¤šä½™ç©ºç™½ï¼‰
        lines = (line.strip() for line in text_content.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text_content = ' '.join(chunk for chunk in chunks if chunk)
        
        # æå–å›¾ç‰‡URL - æ”¹è¿›ç‰ˆæœ¬
        image_urls = []
        
        # å¤„ç†arXivç½‘ç«™çš„å›¾ç‰‡
        if 'arxiv.org' in url:
            # arXivçš„å›¾ç‰‡é€šå¸¸åœ¨ç‰¹å®šçš„ä½ç½®
            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„å›¾ç‰‡å…ƒç´ 
            img_elements = soup.find_all('img')
            
            for img in img_elements:
                src = img.get('src')
                if src:
                    # å¤„ç†ç›¸å¯¹è·¯å¾„
                    if src.startswith('/'):
                        # å¯¹äºarXivï¼Œå›¾ç‰‡é€šå¸¸åœ¨ /html/ è·¯å¾„ä¸‹
                        if '/html/' in src:
                            absolute_url = f"https://arxiv.org{src}"
                        else:
                            absolute_url = urljoin(url, src)
                    elif src.startswith('http'):
                        absolute_url = src
                    else:
                        absolute_url = urljoin(url, src)
                    
                    # éªŒè¯URLæ˜¯å¦ä¸ºæœ‰æ•ˆçš„å›¾ç‰‡é“¾æ¥
                    if is_valid_image_url(absolute_url):
                        image_urls.append(absolute_url)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–å›¾ç‰‡å¼•ç”¨
            if not image_urls:
                # æŸ¥æ‰¾æ–‡æœ¬ä¸­çš„å›¾ç‰‡å¼•ç”¨ï¼Œå¦‚ "Figure 1", "Fig. 2" ç­‰
                import re
                figure_patterns = [
                    r'Figure\s+\d+',
                    r'Fig\.\s*\d+',
                    r'å›¾\s*\d+',
                    r'å›¾è¡¨\s*\d+'
                ]
                
                figures_found = []
                for pattern in figure_patterns:
                    matches = re.findall(pattern, text_content, re.IGNORECASE)
                    figures_found.extend(matches)
                
                if figures_found:
                    print(f"å‘ç°å›¾ç‰‡å¼•ç”¨ï¼š{figures_found[:5]}...")
        else:
            # å¯¹äºå…¶ä»–ç½‘ç«™ï¼Œä½¿ç”¨é€šç”¨æ–¹æ³•
            for img in soup.find_all('img'):
                src = img.get('src')
                if src:
                    absolute_url = urljoin(url, src)
                    if is_valid_image_url(absolute_url):
                        image_urls.append(absolute_url)
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        image_urls = list(set(image_urls))[:100]  # æœ€å¤š10å¼ å›¾ç‰‡
        
        # ä¿å­˜çˆ¬å–ç»“æœåˆ°æœ¬åœ°æ–‡ä»¶
        scraped_data = {
            "url": url,
            "text_content": text_content,
            "image_urls": image_urls,
            "timestamp": str(datetime.datetime.now())
        }
        
        with open("scraped_data.json", "w", encoding="utf-8") as f:
            json.dump(scraped_data, f, ensure_ascii=False, indent=2)
        
        print(f"çˆ¬å–å®Œæˆï¼æ–‡å­—å†…å®¹é•¿åº¦ï¼š{len(text_content)}å­—ç¬¦ï¼Œå›¾ç‰‡æ•°é‡ï¼š{len(image_urls)}")
        if image_urls:
            print("å›¾ç‰‡é“¾æ¥ç¤ºä¾‹ï¼š")
            for i, img_url in enumerate(image_urls[:3], 1):
                print(f"  {i}. {img_url}")
        print(f"æ•°æ®å·²ä¿å­˜åˆ°ï¼š{os.path.abspath('scraped_data.json')}")
        
        return {
            "scraped_text": text_content,
            "image_urls": image_urls
        }
        
    except Exception as e:
        print(f"çˆ¬å–å¤±è´¥ï¼š{e}")
        return {
            "scraped_text": f"çˆ¬å–å¤±è´¥ï¼š{e}",
            "image_urls": []
        }

def arxiv_png_crawler(state: PPTState):
    """çˆ¬å–URLçš„PNGå›¾ç‰‡"""
    url = state["content_url"]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml'
    }
    
    if not url.startswith(('http://', 'https://')):
        return {"png_images": [], "temp_filename": None}
    
    max_retries = 3
    timeout = 15
    retry_delay = 2
    
    for attempt in range(max_retries):
        try:
            response = requests.get(url, headers=headers, timeout=timeout)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            img_tags = soup.find_all('img', {'src': lambda x: x and x.lower().endswith('.png')})
            
            base_url = url if url.endswith('/') else url + '/'
            png_urls = [urljoin(base_url, img['src']) for img in img_tags]
            
            # å»é‡
            png_urls = list(dict.fromkeys(png_urls))
            
            print(f"çˆ¬å–åˆ° {len(png_urls)} å¼ PNGå›¾ç‰‡")
            
            # ç”Ÿæˆä¸´æ—¶JSONæ–‡ä»¶å­˜å‚¨å›¾ç‰‡ä¿¡æ¯
            temp_images_data = {
                "url": url,
                "png_images": png_urls,
                "image_count": len(png_urls),
                "crawl_time": str(datetime.datetime.now()),
                "image_details": []
            }
            
            # ä¸ºæ¯å¼ å›¾ç‰‡æ·»åŠ è¯¦ç»†ä¿¡æ¯
            for i, img_url in enumerate(png_urls, 1):
                img_info = {
                    "index": i,
                    "url": img_url,
                    "filename": os.path.basename(img_url),
                    "status": "pending"  # å¯ä»¥åç»­æ·»åŠ ä¸‹è½½çŠ¶æ€
                }
                temp_images_data["image_details"].append(img_info)
            
            # ä¿å­˜ä¸´æ—¶JSONæ–‡ä»¶
            temp_filename = f"temp_images_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(temp_filename, "w", encoding="utf-8") as f:
                json.dump(temp_images_data, f, ensure_ascii=False, indent=2)
            
            print(f"å›¾ç‰‡ä¿¡æ¯å·²ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶ï¼š{temp_filename}")
            
            return {"png_images": png_urls, "temp_filename": temp_filename}
            
        except requests.exceptions.RequestException as e:
            if attempt == max_retries - 1:
                print(f"çˆ¬å–PNGå›¾ç‰‡å¤±è´¥ï¼š{e}")
                return {"png_images": [], "temp_filename": None}
            time.sleep(retry_delay)
    
    return {"png_images": [], "temp_filename": None}

def is_valid_image_url(url: str) -> bool:
    """éªŒè¯URLæ˜¯å¦ä¸ºæœ‰æ•ˆçš„å›¾ç‰‡é“¾æ¥"""
    # æ£€æŸ¥URLæ˜¯å¦åŒ…å«å¸¸è§çš„å›¾ç‰‡æ‰©å±•å
    image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp', '.svg']
    url_lower = url.lower()
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡æ‰©å±•å
    has_image_ext = any(ext in url_lower for ext in image_extensions)
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡ç›¸å…³çš„è·¯å¾„
    image_paths = ['/image', '/img', '/picture', '/photo', '/figure', '/fig']
    has_image_path = any(path in url_lower for path in image_paths)
    
    # æ’é™¤ä¸€äº›æ˜æ˜¾ä¸æ˜¯å›¾ç‰‡çš„URL
    exclude_patterns = ['logo', 'icon', 'avatar', 'button', 'banner']
    is_excluded = any(pattern in url_lower for pattern in exclude_patterns)
    
    return (has_image_ext or has_image_path) and not is_excluded

def text_summarizer(state: PPTState):
    """ä½¿ç”¨LLMæ€»ç»“æ–‡å­—å†…å®¹"""
    text_content = state["scraped_text"]
    
    if not text_content or "çˆ¬å–å¤±è´¥" in text_content:
        return {"text_summary": "æ— æ³•æ€»ç»“ï¼šå†…å®¹çˆ¬å–å¤±è´¥"}
    
    # å¦‚æœå†…å®¹å¤ªé•¿ï¼Œæˆªå–å‰5000å­—ç¬¦
    if len(text_content) > 5000:
        text_content = text_content[:5000] + "...[å†…å®¹å·²æˆªå–]"
    
    system_prompt = '''ä½ æ˜¯ä¸€ä¸ªå­¦æœ¯è®ºæ–‡æ€»ç»“åŠ©æ‰‹ã€‚è¯·ç”¨ä¸­æ–‡æ€»ç»“è®ºæ–‡å†…å®¹ï¼Œè¦æ±‚å¦‚ä¸‹ï¼š
1. åŒ…å«åŸºæœ¬ä¿¡æ¯ï¼šæ ‡é¢˜ã€ä½œè€…ã€æœºæ„ç­‰
2. æŒ‰ç« èŠ‚ç»“æ„æ€»ç»“ï¼Œæ¯éƒ¨åˆ†çº¦200å­—
3. ä¿æŒå®¢è§‚æ€§ï¼Œä½¿ç”¨å­¦æœ¯è¯­è¨€
4. é¿å…æ•°å­¦å…¬å¼å’Œä»£ç å—
5. æ ‡æ³¨é‡è¦æ•°æ®å’Œå›¾è¡¨å¼•ç”¨
6. åœ¨æœ€åæ³¨æ˜åŸå§‹æ¥æº

æ ¼å¼è¦æ±‚ï¼š
- ä½¿ç”¨äºŒçº§æ ‡é¢˜ï¼ˆ##ï¼‰åˆ’åˆ†ç« èŠ‚
- ä¿æŒå…³é”®æœ¯è¯­çš„è‹±æ–‡åŸæ–‡ï¼Œå¿…è¦æ—¶åŠ ä¸­æ–‡ç¿»è¯‘
- ä»¥è¦ç‚¹å½¢å¼å‘ˆç°æ ¸å¿ƒè´¡çŒ®
- ä¿æŒå¥å­ç®€æ´ï¼Œå»é™¤å†—ä½™è®ºè¯
- åœ¨æœ€åæ·»åŠ "æ¥æºï¼šè®ºæ–‡æ ‡é¢˜"'''

    user_prompt = f"""è¯·åŸºäºä»¥ä¸‹è®ºæ–‡å†…å®¹ç”Ÿæˆç»“æ„åŒ–çš„ä¸­æ–‡æ‘˜è¦ï¼š

{text_content}"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    summary = qwen_chat(messages)
    
    return {"text_summary": summary}

def generate_paper_introduction(url: str) -> str:
    """ä¸»å‡½æ•°ï¼šè¾“å…¥è®ºæ–‡é“¾æ¥ï¼Œç”Ÿæˆè®ºæ–‡ä»‹ç»"""
    print(f"å¼€å§‹å¤„ç†è®ºæ–‡é“¾æ¥ï¼š{url}")
    
    # åˆå§‹åŒ–çŠ¶æ€
    state = PPTState(
        content_url=url,
        scraped_text="",
        image_urls=[],
        text_summary="",
        paper_title="æœªçŸ¥è®ºæ–‡", # åˆå§‹åŒ–è®ºæ–‡æ ‡é¢˜
        png_images=[], # åˆå§‹åŒ–PNGå›¾ç‰‡åˆ—è¡¨
        temp_filename="" # åˆå§‹åŒ–ä¸´æ—¶æ–‡ä»¶å
    )
    
    # ç¬¬ä¸€æ­¥ï¼šçˆ¬å–ç½‘é¡µå†…å®¹
    print("æ­£åœ¨çˆ¬å–ç½‘é¡µå†…å®¹...")
    scraped_result = web_scraper(state)
    state.update(scraped_result)
    
    # ç¬¬äºŒæ­¥ï¼šçˆ¬å–PNGå›¾ç‰‡
    print("æ­£åœ¨çˆ¬å–PNGå›¾ç‰‡...")
    png_result = arxiv_png_crawler(state)
    state.update(png_result)

    # ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆæ€»ç»“
    print("æ­£åœ¨ç”Ÿæˆè®ºæ–‡æ€»ç»“...")
    summary_result = text_summarizer(state)
    state.update(summary_result)
    
    # æå–è®ºæ–‡æ ‡é¢˜
    state["paper_title"] = extract_paper_title(state["scraped_text"], url)
    print(f"æå–åˆ°çš„è®ºæ–‡æ ‡é¢˜ï¼š{state['paper_title']}")

    # ä¿å­˜æœ€ç»ˆç»“æœåˆ°JSON
    final_result = {
        "url": url,
        "summary": state["text_summary"],
        "image_count": len(state["image_urls"]),
        "png_image_count": len(state["png_images"]),
        "total_images": len(state["image_urls"]) + len(state["png_images"]),
        "image_urls": state["image_urls"],
        "png_images": state["png_images"],
        "timestamp": str(datetime.datetime.now())
    }
    
    # åˆ›å»ºpaper_outputæ–‡ä»¶å¤¹
    output_dir = "paper_output"
    os.makedirs(output_dir, exist_ok=True)

    # ä½¿ç”¨è®ºæ–‡æ ‡é¢˜ä½œä¸ºæ–‡ä»¶å
    output_filename = f"{state['paper_title']}.json"
    with open(os.path.join(output_dir, output_filename), "w", encoding="utf-8") as f:
        json.dump(final_result, f, ensure_ascii=False, indent=2)
    
    # ç”ŸæˆHTMLæŠ¥å‘Š
    html_path = generate_html_report(url, state["text_summary"], state["image_urls"], state["scraped_text"], state["paper_title"], state["png_images"])
    
    # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
    if state.get("temp_filename"):
        try:
            os.remove(state["temp_filename"])
            print(f"ä¸´æ—¶æ–‡ä»¶ {state['temp_filename']} å·²åˆ é™¤ã€‚")
        except OSError as e:
            print(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥ï¼š{e}")
    
    print(f"\nè®ºæ–‡æ€»ç»“å·²ä¿å­˜åˆ°ï¼š{os.path.abspath(os.path.join(output_dir, output_filename))}")
    print(f"HTMLæŠ¥å‘Šå·²ä¿å­˜åˆ°ï¼š{os.path.abspath(html_path)}")
    print("\n" + "="*50)
    print("è®ºæ–‡æ€»ç»“ï¼š")
    print("="*50)
    print(state["text_summary"])
    
    return state["text_summary"]

def generate_html_report(url: str, summary: str, image_urls: List[str], original_text: str, paper_title: str = "æœªçŸ¥è®ºæ–‡", png_images: List[str] = None):
    """ç”ŸæˆåŒ…å«å›¾ç‰‡çš„HTMLæŠ¥å‘Š"""
    
    if png_images is None:
        png_images = []
    
    # ç”Ÿæˆå›¾ç‰‡HTML - åªä½¿ç”¨PNGå›¾ç‰‡
    images_html = ""
    
    if png_images:
        images_html = '<div class="images-section">\n<h2>ğŸ“· è®ºæ–‡PNGå›¾ç‰‡</h2>\n<div class="image-gallery">\n'
        
        # æ˜¾ç¤ºPNGå›¾ç‰‡ï¼ˆé™åˆ¶20å¼ ï¼‰
        for i, img_url in enumerate(png_images[:20], 1):
            # å°è¯•ä»URLä¸­æå–å›¾ç‰‡åç§°
            img_name = f"PNGå›¾ç‰‡ {i}"
            
            # ä»URLè·¯å¾„ä¸­æå–æ–‡ä»¶å
            path_parts = img_url.split('/')
            if len(path_parts) > 1:
                filename = path_parts[-1]
                if '.' in filename:
                    img_name = filename.split('.')[0]
            
            # å°è¯•ä»æ–‡ä»¶åä¸­æå–å›¾ç‰‡ç¼–å·
            import re
            fig_match = re.search(r'x(\d+)', img_name)
            if fig_match:
                img_name = f"Figure {fig_match.group(1)}"
            
            images_html += f'''
            <div class="image-item">
                <img src="{img_url}" alt="{img_name}" onerror="this.style.display='none'; this.nextElementSibling.innerHTML='å›¾ç‰‡åŠ è½½å¤±è´¥'">
                <p class="image-caption">{img_name} (PNG)</p>
            </div>'''
        images_html += '</div>\n</div>\n'
    
    # ç”ŸæˆHTMLå†…å®¹
    html_content = f'''<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>è®ºæ–‡é˜…è¯»æŠ¥å‘Š</title>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
            color: #333;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }}
        .header {{
            text-align: center;
            border-bottom: 2px solid #007acc;
            padding-bottom: 20px;
            margin-bottom: 30px;
        }}
        .header h1 {{
            color: #007acc;
            margin: 0;
            font-size: 2.5em;
        }}
        .meta-info {{
            background: #f8f9fa;
            padding: 15px;
            border-radius: 5px;
            margin-bottom: 20px;
            border-left: 4px solid #007acc;
        }}
        .meta-info p {{
            margin: 5px 0;
        }}
        .summary-section {{
            margin-bottom: 30px;
        }}
        .summary-section h2 {{
            color: #007acc;
            border-bottom: 1px solid #ddd;
            padding-bottom: 10px;
        }}
        .summary-content {{
            background: #fafafa;
            padding: 20px;
            border-radius: 5px;
            white-space: pre-wrap;
            line-height: 1.8;
        }}
        .images-section {{
            margin-top: 30px;
        }}
        .images-section h2 {{
            color: #007acc;
            border-bottom: 1px solid #ddd;
            padding-bottom: 10px;
        }}
        .image-gallery {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin-top: 20px;
        }}
        .image-item {{
            text-align: center;
            background: #f8f9fa;
            padding: 15px;
            border-radius: 8px;
            border: 1px solid #ddd;
        }}
        .image-item img {{
            max-width: 100%;
            height: auto;
            border-radius: 5px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }}
        .image-caption {{
            margin-top: 10px;
            font-size: 0.9em;
            color: #666;
        }}
        .original-text {{
            margin-top: 30px;
            padding: 20px;
            background: #f8f9fa;
            border-radius: 5px;
            max-height: 400px;
            overflow-y: auto;
            border: 1px solid #ddd;
        }}
        .original-text h3 {{
            color: #007acc;
            margin-top: 0;
        }}
        .footer {{
            text-align: center;
            margin-top: 30px;
            padding-top: 20px;
            border-top: 1px solid #ddd;
            color: #666;
        }}
        @media (max-width: 768px) {{
            .container {{
                padding: 15px;
            }}
            .image-gallery {{
                grid-template-columns: 1fr;
            }}
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ“š è®ºæ–‡é˜…è¯»æŠ¥å‘Š</h1>
            <p>AIæ™ºèƒ½è®ºæ–‡åˆ†æå·¥å…·ç”Ÿæˆ</p>
        </div>
        
        <div class="meta-info">
            <p><strong>ğŸ“„ è®ºæ–‡é“¾æ¥ï¼š</strong><a href="{url}" target="_blank">{url}</a></p>
            <p><strong>ğŸ“… ç”Ÿæˆæ—¶é—´ï¼š</strong>{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
        
        <div class="summary-section">
            <h2>ğŸ“ è®ºæ–‡æ‘˜è¦</h2>
            <div class="summary-content">{summary}</div>
        </div>
        
        {images_html}
        
        <div class="original-text">
            <h3>ğŸ“„ åŸå§‹æ–‡æœ¬ï¼ˆå‰1000å­—ç¬¦ï¼‰</h3>
            <p>{original_text[:1000]}{'...' if len(original_text) > 1000 else ''}</p>
        </div>
        
        <div class="footer">
            <p>ç”±è®ºæ–‡é˜…è¯»å·¥å…·ç”Ÿæˆ | åŸºäºQwen AIæŠ€æœ¯</p>
        </div>
    </div>
</body>
</html>'''
    
    # åˆ›å»ºpaper_outputæ–‡ä»¶å¤¹
    output_dir = "paper_output"
    os.makedirs(output_dir, exist_ok=True)
    
    # ä½¿ç”¨è®ºæ–‡æ ‡é¢˜ä½œä¸ºHTMLæ–‡ä»¶å
    html_filename = f"{paper_title}.html"
    html_path = os.path.join(output_dir, html_filename)
    
    # ä¿å­˜HTMLæ–‡ä»¶
    with open(html_path, "w", encoding="utf-8") as f:
        f.write(html_content)
    
    return html_path

if __name__ == "__main__":
    # ç¤ºä¾‹ä½¿ç”¨
    paper_url = input("è¯·è¾“å…¥è®ºæ–‡é“¾æ¥ï¼š")
    if paper_url.strip():
        generate_paper_introduction(paper_url)
    else:
        print("æœªè¾“å…¥æœ‰æ•ˆé“¾æ¥")

